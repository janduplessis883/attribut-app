{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae342416-0758-46d5-a49e-777364ca7c98",
   "metadata": {},
   "source": [
    "# Fake Dataset and RL Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b5340529-a2f1-42e9-87eb-4c42298f8b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "df = pd.read_csv('/Users/janduplessis/code/janduplessis883/attribut-app/data/synthetic_dataset2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d08027c8-cfad-4b8a-9a77-f47ef451e516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>next_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'tasks': [{'id': 0, 'name': 'monetize efficie...</td>\n",
       "      <td>{'task_id': 6, 'time_block': (datetime.datetim...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'tasks': [{'id': 0, 'name': 'monetize efficie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'tasks': [{'id': 0, 'name': 'empower customiz...</td>\n",
       "      <td>{'task_id': 1, 'time_block': (datetime.datetim...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'tasks': [{'id': 0, 'name': 'empower customiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'tasks': [{'id': 0, 'name': 'maximize 24/7 we...</td>\n",
       "      <td>{'task_id': 3, 'time_block': (datetime.datetim...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>{'tasks': [{'id': 0, 'name': 'maximize 24/7 we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'tasks': [{'id': 0, 'name': 'productize B2C m...</td>\n",
       "      <td>{'task_id': 0, 'time_block': (datetime.datetim...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'tasks': [{'id': 1, 'name': 'incubate strateg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'tasks': [{'id': 0, 'name': 'brand 24/365 syn...</td>\n",
       "      <td>{'task_id': 1, 'time_block': (datetime.datetim...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'tasks': [{'id': 0, 'name': 'brand 24/365 syn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               state  \\\n",
       "0  {'tasks': [{'id': 0, 'name': 'monetize efficie...   \n",
       "1  {'tasks': [{'id': 0, 'name': 'empower customiz...   \n",
       "2  {'tasks': [{'id': 0, 'name': 'maximize 24/7 we...   \n",
       "3  {'tasks': [{'id': 0, 'name': 'productize B2C m...   \n",
       "4  {'tasks': [{'id': 0, 'name': 'brand 24/365 syn...   \n",
       "\n",
       "                                              action  reward  \\\n",
       "0  {'task_id': 6, 'time_block': (datetime.datetim...     0.1   \n",
       "1  {'task_id': 1, 'time_block': (datetime.datetim...     0.1   \n",
       "2  {'task_id': 3, 'time_block': (datetime.datetim...     0.2   \n",
       "3  {'task_id': 0, 'time_block': (datetime.datetim...     0.1   \n",
       "4  {'task_id': 1, 'time_block': (datetime.datetim...     0.1   \n",
       "\n",
       "                                          next_state  \n",
       "0  {'tasks': [{'id': 0, 'name': 'monetize efficie...  \n",
       "1  {'tasks': [{'id': 0, 'name': 'empower customiz...  \n",
       "2  {'tasks': [{'id': 0, 'name': 'maximize 24/7 we...  \n",
       "3  {'tasks': [{'id': 1, 'name': 'incubate strateg...  \n",
       "4  {'tasks': [{'id': 0, 'name': 'brand 24/365 syn...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fe14c2af-d2eb-4331-a5eb-411188100c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'tasks': [{'id': 0, 'name': 'empower customized channels', 'deadline': datetime.datetime(2025, 2, 11, 0, 12, 43, 449336), 'priority': 2, 'duration': 30, 'dependencies': []}, {'id': 1, 'name': 'redefine revolutionary models', 'deadline': datetime.datetime(2025, 2, 11, 11, 12, 2, 563684), 'priority': 3, 'duration': 30, 'dependencies': [0], 'type': 'meeting', 'scheduled_time': (datetime.datetime(2025, 2, 4, 11, 15), datetime.datetime(2025, 2, 4, 11, 45))}, {'id': 2, 'name': 'target compelling users', 'deadline': datetime.datetime(2025, 2, 11, 11, 27, 45, 325533), 'priority': 2, 'duration': 30, 'dependencies': [0]}], 'calendar': [{'start': datetime.datetime(2025, 2, 4, 12, 30), 'end': datetime.datetime(2025, 2, 4, 13, 0), 'type': 'meeting'}, {'start': datetime.datetime(2025, 2, 4, 13, 0), 'end': datetime.datetime(2025, 2, 4, 14, 0), 'type': 'meeting'}], 'preferences': {'productive_hours': [11, 18], 'work_days': [0, 1, 2, 3, 4]}}\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['state'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b2f9c02e-cbfb-4985-aac4-64cdf9773cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'task_id': 6, 'time_block': (datetime.datetime(2025, 2, 4, 14, 30), datetime.datetime(2025, 2, 4, 16, 0))}\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['action'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d937da2d-7af9-4a83-9ff9-59613e500374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'tasks': [{'id': 0, 'name': 'maximize 24/7 web-readiness', 'deadline': datetime.datetime(2025, 2, 7, 0, 42, 20, 473396), 'priority': 1, 'duration': 90, 'dependencies': []}, {'id': 1, 'name': 'integrate strategic markets', 'deadline': datetime.datetime(2025, 2, 9, 15, 1, 10, 848200), 'priority': 3, 'duration': 30, 'dependencies': []}, {'id': 2, 'name': 'envisioneer open-source models', 'deadline': datetime.datetime(2025, 2, 11, 11, 20, 59, 135046), 'priority': 3, 'duration': 60, 'dependencies': []}, {'id': 4, 'name': 'evolve open-source solutions', 'deadline': datetime.datetime(2025, 2, 6, 0, 40, 43, 960830), 'priority': 3, 'duration': 30, 'dependencies': [3, 2], 'type': 'meeting'}, {'id': 5, 'name': 'extend enterprise models', 'deadline': datetime.datetime(2025, 2, 10, 11, 41, 18, 852463), 'priority': 2, 'duration': 30, 'dependencies': []}, {'id': 6, 'name': 'transition bleeding-edge portals', 'deadline': datetime.datetime(2025, 2, 6, 10, 25, 1, 957652), 'priority': 1, 'duration': 90, 'dependencies': [2]}], 'calendar': [{'start': datetime.datetime(2025, 2, 4, 7, 0), 'end': datetime.datetime(2025, 2, 4, 8, 0), 'type': 'break'}, {'start': datetime.datetime(2025, 2, 4, 8, 30), 'end': datetime.datetime(2025, 2, 4, 9, 30), 'type': 'break'}, {'start': datetime.datetime(2025, 2, 4, 14, 0), 'end': datetime.datetime(2025, 2, 4, 14, 30), 'type': 'call'}, {'start': datetime.datetime(2025, 2, 4, 14, 30), 'end': datetime.datetime(2025, 2, 4, 15, 0), 'type': 'meeting'}, {'start': datetime.datetime(2025, 2, 4, 17, 0), 'end': datetime.datetime(2025, 2, 4, 17, 30), 'type': 'scheduled_task', 'task_id': 3, 'name': 'scale rich architectures'}], 'preferences': {'productive_hours': [6, 22], 'work_days': [0, 1, 2, 3, 4]}}\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['next_state'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f0659-1768-41bc-8efb-0b2a6eb16307",
   "metadata": {},
   "source": [
    "PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "30bc0ee4-83c6-47b1-955b-9c5736c42cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            features  action_label\n",
      "0  [-0.9426414910921783, -0.33380685923377124, -0...           317\n",
      "1  [-0.9426414910921783, -0.33380685923377124, 0....            70\n",
      "2  [-0.9426414910921783, -0.33380685923377124, 0....           178\n",
      "3  [-0.9426414910921783, -0.33380685923377124, 0....            39\n",
      "4  [-0.9426414910921783, -0.33380685923377124, 0....            76\n",
      "Training Data Loader Ready\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime, timedelta\n",
    "import datetime as dt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Helper Functions\n",
    "# -------------------------------\n",
    "def safe_eval(x):\n",
    "    \"\"\"\n",
    "    Evaluate a string containing a dictionary with datetime objects.\n",
    "    Provides the datetime module and timedelta for evaluation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use the datetime module from dt so that \"datetime.datetime(...)\" works.\n",
    "        return eval(x, {\"datetime\": dt, \"timedelta\": dt.timedelta})\n",
    "    except Exception as e:\n",
    "        print(\"Error evaluating string:\", e)\n",
    "        return x\n",
    "\n",
    "def state_to_features(state, max_tasks=10, time_slots=48):\n",
    "    \"\"\"\n",
    "    Convert the state dictionary into a numerical feature vector.\n",
    "\n",
    "    Feature breakdown:\n",
    "      - 2 features: sine & cosine of current time-of-day.\n",
    "      - 1 feature: normalized productive hours remaining.\n",
    "      - max_tasks * 6 features: for each task, include:\n",
    "            * normalized priority,\n",
    "            * days remaining until deadline,\n",
    "            * normalized duration (relative to 4 hours),\n",
    "            * normalized count of dependencies,\n",
    "            * binary flag if type contains 'meeting',\n",
    "            * binary flag if the task is already scheduled.\n",
    "      - time_slots features: one-hot encoding of occupied calendar time slots (30-minute slots).\n",
    "    \"\"\"\n",
    "    now = datetime.now()\n",
    "    # Total feature length = 3 (time context) + (max_tasks*6) + time_slots\n",
    "    features = np.zeros(3 + max_tasks * 6 + time_slots)\n",
    "\n",
    "    # 1. Time of day (sine/cosine encoding)\n",
    "    hour = now.hour + now.minute / 60.0\n",
    "    features[0] = np.sin(2 * np.pi * hour / 24)\n",
    "    features[1] = np.cos(2 * np.pi * hour / 24)\n",
    "\n",
    "    # 2. Productive hours remaining (normalized over 24 hours)\n",
    "    productive_start, productive_end = state['preferences']['productive_hours']\n",
    "    features[2] = (productive_end - hour) / 24.0\n",
    "\n",
    "    # 3. Task features (up to max_tasks)\n",
    "    for i, task in enumerate(state['tasks'][:max_tasks]):\n",
    "        offset = 3 + i * 6\n",
    "        # Normalize priority (assuming values 1-3)\n",
    "        features[offset] = task['priority'] / 3.0\n",
    "        # Days until deadline (could be negative if past due)\n",
    "        features[offset + 1] = (task['deadline'] - now).total_seconds() / 86400.0\n",
    "        # Duration normalized to max 4 hours (240 minutes)\n",
    "        features[offset + 2] = task['duration'] / 240.0\n",
    "        # Number of dependencies normalized (assuming maximum of 5)\n",
    "        features[offset + 3] = len(task['dependencies']) / 5.0\n",
    "        # Binary flag: does the task type contain 'meeting'?\n",
    "        features[offset + 4] = 1 if 'meeting' in task.get('type', '') else 0\n",
    "        # Binary flag: has the task been scheduled?\n",
    "        features[offset + 5] = 1 if task.get('scheduled_time') else 0\n",
    "\n",
    "    # 4. Calendar time slots (48 slots for 30-minute intervals in a day)\n",
    "    base_index = 3 + max_tasks * 6\n",
    "    for event in state['calendar']:\n",
    "        start = event['start']\n",
    "        end = event['end']\n",
    "        start_slot = int((start.hour * 60 + start.minute) / 30)\n",
    "        duration_slots = int((end - start).total_seconds() / 1800)\n",
    "        for j in range(start_slot, min(start_slot + duration_slots, time_slots)):\n",
    "            features[base_index + j] = 1\n",
    "\n",
    "    return features\n",
    "\n",
    "def action_to_label(action, time_slots=48):\n",
    "    \"\"\"\n",
    "    Convert the action into a discrete label.\n",
    "    Label is computed as: task_id * time_slots + time_slot index.\n",
    "    \"\"\"\n",
    "    start_time = action['time_block'][0]\n",
    "    time_slot = int((start_time.hour * 60 + start_time.minute) / 30)\n",
    "    return action['task_id'] * time_slots + time_slot\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Process DataFrame\n",
    "# -------------------------------\n",
    "# Assuming you have already loaded your DataFrame `df` from a CSV.\n",
    "# For example: df = pd.read_csv(\"your_dataset.csv\")\n",
    "# The DataFrame should contain the columns: 'state', 'action', 'reward', 'next_state'.\n",
    "\n",
    "# Convert string representations to dictionaries.\n",
    "df['state'] = df['state'].apply(safe_eval)\n",
    "df['action'] = df['action'].apply(safe_eval)\n",
    "df['next_state'] = df['next_state'].apply(safe_eval)\n",
    "\n",
    "# Compute features for the state and next state, and compute discrete action labels.\n",
    "df['features'] = df['state'].apply(state_to_features)\n",
    "df['next_features'] = df['next_state'].apply(state_to_features)\n",
    "df['action_label'] = df['action'].apply(action_to_label)\n",
    "\n",
    "# Quick check of the features and action labels.\n",
    "print(df[['features', 'action_label']].head())\n",
    "\n",
    "# Convert the lists in the DataFrame to NumPy arrays.\n",
    "observations = np.stack(df['features'].values)         # shape: [num_samples, feature_dim]\n",
    "next_observations = np.stack(df['next_features'].values) # shape: [num_samples, feature_dim]\n",
    "action_labels = df['action_label'].values                # shape: [num_samples]\n",
    "rewards = df['reward'].values.astype(np.float32)         # shape: [num_samples]\n",
    "# Mark each transition as terminal (modify if you have a \"done\" flag in your data).\n",
    "dones = np.ones_like(rewards, dtype=bool)\n",
    "\n",
    "# Split into training and validation sets.\n",
    "indices = np.arange(len(df))\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Create a PyTorch Dataset\n",
    "# -------------------------------\n",
    "class RLTransitionDataset(Dataset):\n",
    "    def __init__(self, observations, actions, rewards, next_observations, dones):\n",
    "        self.observations = torch.tensor(observations, dtype=torch.float32)\n",
    "        self.actions = torch.tensor(actions, dtype=torch.long)\n",
    "        self.rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        self.next_observations = torch.tensor(next_observations, dtype=torch.float32)\n",
    "        self.dones = torch.tensor(dones, dtype=torch.bool)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.observations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"obs\": self.observations[idx],\n",
    "            \"action\": self.actions[idx],\n",
    "            \"reward\": self.rewards[idx],\n",
    "            \"next_obs\": self.next_observations[idx],\n",
    "            \"done\": self.dones[idx],\n",
    "        }\n",
    "\n",
    "# Create training and validation datasets.\n",
    "train_dataset = RLTransitionDataset(\n",
    "    observations[train_indices],\n",
    "    action_labels[train_indices],\n",
    "    rewards[train_indices],\n",
    "    next_observations[train_indices],\n",
    "    dones[train_indices]\n",
    ")\n",
    "\n",
    "val_dataset = RLTransitionDataset(\n",
    "    observations[val_indices],\n",
    "    action_labels[val_indices],\n",
    "    rewards[val_indices],\n",
    "    next_observations[val_indices],\n",
    "    dones[val_indices]\n",
    ")\n",
    "\n",
    "# Create DataLoaders.\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"Training Data Loader Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17cd0e3-e457-4def-8ff0-de40924b404d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016f20a-91d0-480d-942c-6cc42a35132e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d02d4cdd-e893-441a-91c7-5f32b931fa83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'obs': tensor([[0.9958, 0.0915, 0.5562,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.9958, 0.0915, 0.5146,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.9958, 0.0915, 0.5562,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.9958, 0.0915, 0.6396,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.9958, 0.0915, 0.4729,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.9958, 0.0915, 0.6396,  ..., 0.0000, 0.0000, 0.0000]]), 'action': tensor([129,  62, 218, 187, 118,  71, 170,  87,  24, 308, 125,  36, 259, 128,\n",
      "         61,  34,  27,  23, 214, 226, 160,  78, 156, 261,  74, 263,  27, 314,\n",
      "        213,  22, 167,  77,  25,  19, 208,  25,  25, 120,  37, 122,  17, 170,\n",
      "        167, 114, 352,  77, 119,  75,  36,  29, 123, 137, 279,  26, 263,  71,\n",
      "        113,  85, 170, 267, 174, 165,  23,  34]), 'reward': tensor([ 3.0000e-01,  1.0000e-01,  7.0000e-02,  3.6667e-02,  2.0000e-01,\n",
      "         1.0000e-01,  1.4000e-01,  1.0000e-01,  2.0000e-01,  1.4000e-01,\n",
      "         1.0667e-01,  1.0000e-01,  7.0000e-02,  7.0000e-02,  1.0000e-01,\n",
      "         7.0000e-02,  1.0000e-01,  1.7000e-01,  3.0000e-01,  2.7000e-01,\n",
      "         1.0000e-01,  4.5000e-02,  1.0000e-01,  2.7000e-01,  7.0000e-02,\n",
      "         8.0000e-02,  3.6667e-02,  3.0000e-01,  1.7000e-01,  1.0277e-02,\n",
      "         2.4000e-01,  1.0000e-01,  1.0000e-02,  1.7000e-01,  2.7000e-01,\n",
      "         1.7000e-01,  1.7000e-01,  1.0000e-01,  2.5000e-02,  2.4000e-01,\n",
      "         1.0000e-01,  1.7000e-01,  1.7000e-01,  1.7000e-01,  7.0000e-02,\n",
      "         5.0000e-02,  1.0000e-01,  3.0004e-01,  3.3333e-02,  3.0000e-01,\n",
      "         1.4001e-01, -1.3878e-17,  7.0000e-02,  1.7000e-01,  2.0000e-01,\n",
      "         1.7000e-01,  1.0000e-01,  1.0000e-01,  1.4000e-01,  4.0004e-02,\n",
      "         1.7000e-01,  3.0000e-01,  7.0000e-02,  3.0000e-01]), 'done': tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True]), 'next_obs': tensor([[0.9962, 0.0872, 0.5556,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.9962, 0.0872, 0.5139,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.9962, 0.0872, 0.5556,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.9962, 0.0872, 0.6389,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.9962, 0.0872, 0.4722,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.9962, 0.0872, 0.6389,  ..., 0.0000, 0.0000, 0.0000]])}\n",
      "Loaded 10000 transitions from offline_data.jsonl.\n",
      "DataLoader created. Ready to train the model.\n",
      "\n",
      "Using device: cpu\n",
      "\n",
      "Starting training loop...\n",
      "\n",
      "Epoch 1/10: Average Loss = 5.6140\n",
      "Epoch 2/10: Average Loss = 5.1602\n",
      "Epoch 3/10: Average Loss = 4.7775\n",
      "Epoch 4/10: Average Loss = 4.1214\n",
      "Epoch 5/10: Average Loss = 3.7589\n",
      "Epoch 6/10: Average Loss = 3.5908\n",
      "Epoch 7/10: Average Loss = 3.4897\n",
      "Epoch 8/10: Average Loss = 3.4269\n",
      "Epoch 9/10: Average Loss = 3.3694\n",
      "Epoch 10/10: Average Loss = 3.3329\n",
      "\n",
      "Training complete.\n",
      "Model saved as offline_cql_model_torch.pth\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TorchOfflineDataset(Dataset):\n",
    "    def __init__(self, jsonl_file):\n",
    "        self.data = []\n",
    "        with open(jsonl_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        # Convert the fields to PyTorch tensors with the appropriate types.\n",
    "        return {\n",
    "            'obs': torch.tensor(sample['obs'], dtype=torch.float32),\n",
    "            'action': torch.tensor(sample['actions'], dtype=torch.long),\n",
    "            'reward': torch.tensor(sample['rewards'], dtype=torch.float32),\n",
    "            'done': torch.tensor(sample['dones'], dtype=torch.bool),\n",
    "            'next_obs': torch.tensor(sample['next_obs'], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# Usage:\n",
    "jsonl_filename = \"offline_data.jsonl\"  # Path to your prepared JSONL dataset.\n",
    "dataset = TorchOfflineDataset(jsonl_filename)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Quick test of the loader:\n",
    "for batch in data_loader:\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Define the Offline Dataset (JSONL based)\n",
    "# -------------------------------\n",
    "class TorchOfflineDataset(Dataset):\n",
    "    def __init__(self, jsonl_file):\n",
    "        self.data = []\n",
    "        with open(jsonl_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "        print(f\"Loaded {len(self.data)} transitions from {jsonl_file}.\")\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        # Convert each field to a PyTorch tensor with the appropriate type.\n",
    "        return {\n",
    "            'obs': torch.tensor(sample['obs'], dtype=torch.float32),\n",
    "            'action': torch.tensor(sample['actions'], dtype=torch.long),\n",
    "            'reward': torch.tensor(sample['rewards'], dtype=torch.float32),\n",
    "            'done': torch.tensor(sample['dones'], dtype=torch.bool),\n",
    "            'next_obs': torch.tensor(sample['next_obs'], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# Specify the JSONL file path for your offline dataset.\n",
    "jsonl_filename = \"offline_data.jsonl\"\n",
    "dataset = TorchOfflineDataset(jsonl_filename)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "print(\"DataLoader created. Ready to train the model.\\n\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Define the Q-Network and Helper Functions\n",
    "# -------------------------------\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, n_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.out = nn.Linear(256, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.out(x)  # returns Q-values for each action\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(tau * source_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Hyperparameters & Setup\n",
    "# -------------------------------\n",
    "obs_dim = 111      # observation dimension (as defined by your environment)\n",
    "n_actions = 480    # total number of discrete actions\n",
    "gamma = 0.99       # discount factor\n",
    "alpha = 1.0        # conservative penalty weight (tune this!)\n",
    "batch_size = 64\n",
    "learning_rate = 3e-4\n",
    "n_epochs = 10\n",
    "target_update_tau = 0.005\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "q_net = QNetwork(obs_dim, n_actions).to(device)\n",
    "target_q_net = QNetwork(obs_dim, n_actions).to(device)\n",
    "target_q_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=learning_rate)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Training Loop (CQL)\n",
    "# -------------------------------\n",
    "print(\"Starting training loop...\\n\")\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    batch_count = 0\n",
    "    for batch in data_loader:\n",
    "        # Prepare batch data as tensors (they are already tensors from our dataset)\n",
    "        obs = batch['obs'].to(device)           # shape: [B, obs_dim]\n",
    "        actions = batch['action'].to(device)      # shape: [B]\n",
    "        rewards = batch['reward'].unsqueeze(1).to(device)  # shape: [B, 1]\n",
    "        # Convert done flag to float (1.0 if done, 0.0 if not) for computation.\n",
    "        dones = batch['done'].float().unsqueeze(1).to(device)  # shape: [B, 1]\n",
    "        next_obs = batch['next_obs'].to(device)   # shape: [B, obs_dim]\n",
    "        \n",
    "        # Compute current Q-values for all actions.\n",
    "        q_values = q_net(obs)  # shape: [B, n_actions]\n",
    "        # Gather Q-values for the taken actions.\n",
    "        q_value = q_values.gather(1, actions.unsqueeze(1))  # shape: [B, 1]\n",
    "        \n",
    "        # Compute target Q-value using the target network (max over actions).\n",
    "        with torch.no_grad():\n",
    "            next_q_values = target_q_net(next_obs)  # shape: [B, n_actions]\n",
    "            next_q_value, _ = next_q_values.max(dim=1, keepdim=True)\n",
    "            target = rewards + gamma * (1 - dones) * next_q_value  # shape: [B, 1]\n",
    "        \n",
    "        # Compute the Bellman error (MSE loss).\n",
    "        bellman_loss = nn.MSELoss()(q_value, target)\n",
    "        \n",
    "        # Conservative penalty term:\n",
    "        # Compute logsumexp over all actions to encourage lower Q-values on out-of-distribution actions.\n",
    "        batch_q = q_net(obs)  # shape: [B, n_actions]\n",
    "        max_q, _ = batch_q.max(dim=1, keepdim=True)\n",
    "        logsumexp = torch.log(torch.sum(torch.exp(batch_q - max_q), dim=1, keepdim=True)) + max_q\n",
    "        conservative_penalty = logsumexp - q_value  # shape: [B, 1]\n",
    "        \n",
    "        loss = bellman_loss + alpha * conservative_penalty.mean()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        batch_count += 1\n",
    "\n",
    "    # Soft update the target network at the end of each epoch.\n",
    "    soft_update(target_q_net, q_net, target_update_tau)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}: Average Loss = {epoch_loss / batch_count:.4f}\")\n",
    "    \n",
    "print(\"\\nTraining complete.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Save the Trained Model\n",
    "# -------------------------------\n",
    "torch.save(q_net.state_dict(), \"offline_cql_model_torch.pth\")\n",
    "print(\"Model saved as offline_cql_model_torch.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae7582d-4cfe-48a8-a5c0-af32782b410c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c8d5d90-d9af-4baa-9cef-14d5317835f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Action Index: 136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gb/zydm3yr102d7ph3f1rz4687c0000gn/T/ipykernel_36312/1420608669.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  q_net.load_state_dict(torch.load(\"offline_cql_model_torch.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assume state_to_features is defined exactly as during training.\n",
    "# And assume your QNetwork class is available.\n",
    "\n",
    "# Define the device and recreate the network architecture.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "obs_dim = 111\n",
    "n_actions = 480\n",
    "\n",
    "# Initialize the network and load the trained weights.\n",
    "q_net = QNetwork(obs_dim, n_actions).to(device)\n",
    "q_net.load_state_dict(torch.load(\"offline_cql_model_torch.pth\", map_location=device))\n",
    "q_net.eval()  # set to evaluation mode\n",
    "\n",
    "def select_action(real_world_state):\n",
    "    \"\"\"\n",
    "    real_world_state: a dictionary with the same structure as the state used during training.\n",
    "    \"\"\"\n",
    "    # Convert your state into features.\n",
    "    features = state_to_features(real_world_state)  # returns a NumPy array of shape [111]\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0).to(device)  # add batch dim\n",
    "\n",
    "    # Forward pass to get Q-values.\n",
    "    with torch.no_grad():\n",
    "        q_values = q_net(features_tensor)  # shape: [1, n_actions]\n",
    "    \n",
    "    # Choose the action with highest Q-value.\n",
    "    action_index = q_values.argmax(dim=1).item()  # greedy selection\n",
    "\n",
    "    # Optional: If you have a mapping function to decode the action index back into a more interpretable action:\n",
    "    # action = decode_action(action_index)\n",
    "    \n",
    "    return action_index\n",
    "\n",
    "# Example usage:\n",
    "# Construct a sample real-world state dictionary that mirrors your training data's structure.\n",
    "# This is just a dummy example. Replace it with actual real-world data.\n",
    "real_world_state = {\n",
    "    \"preferences\": {\n",
    "        \"productive_hours\": (14, 18)  # for instance, 9 AM to 5 PM\n",
    "    },\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"priority\": 1,\n",
    "            \"deadline\": datetime.now() + timedelta(days=1),\n",
    "            \"duration\": 15,  # in minutes\n",
    "            \"dependencies\": [],\n",
    "            \"type\": \"email\",\n",
    "            \"scheduled_time\": None\n",
    "        },\n",
    "        # ... up to however many tasks you want to simulate\n",
    "    ],\n",
    "    \"calendar\": [\n",
    "        {\n",
    "            \"start\": datetime.now() + timedelta(hours=1),\n",
    "            \"end\": datetime.now() + timedelta(hours=2)\n",
    "        },\n",
    "        # Additional calendar events as needed.\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Select an action based on the current real-world state.\n",
    "selected_action = select_action(real_world_state)\n",
    "print(\"Selected Action Index:\", selected_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "01625fe7-8780-40f7-ae86-2a5d307dcd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded action 27: task_id = 0, time_slot = 27\n",
      "Time slot 27 corresponds to 13:30:00 - 14:00:00\n",
      "Task 0 scheduled at 2025-02-04 13:30:00\n",
      "Added calendar event: {'start': datetime.datetime(2025, 2, 4, 13, 30), 'end': datetime.datetime(2025, 2, 4, 14, 0)}\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def decode_action(action_index, time_slots=48):\n",
    "    \"\"\"\n",
    "    Given an encoded action index, decode it to (task_id, time_slot).\n",
    "    \"\"\"\n",
    "    task_id = action_index // time_slots\n",
    "    time_slot = action_index % time_slots\n",
    "    return task_id, time_slot\n",
    "\n",
    "def get_timeslot_start(time_slot, base_time=None):\n",
    "    \"\"\"\n",
    "    Convert a time_slot index to an actual datetime object.\n",
    "    Assumes that the day starts at base_time (default: midnight of today).\n",
    "    Each slot is 30 minutes long.\n",
    "    \"\"\"\n",
    "    if base_time is None:\n",
    "        # Define midnight of the current day.\n",
    "        base_time = datetime.combine(datetime.today(), datetime.min.time())\n",
    "    return base_time + timedelta(minutes=time_slot * 30)\n",
    "\n",
    "def update_task_calendar(state, action_index, time_slots=48):\n",
    "    \"\"\"\n",
    "    Given a state dictionary and an encoded action index, update the state:\n",
    "      - Decode the action to get task_id and time_slot.\n",
    "      - Set the scheduled_time for that task.\n",
    "      - Append a new calendar event for that time slot.\n",
    "    \"\"\"\n",
    "    # Decode the action index.\n",
    "    task_id, time_slot = decode_action(action_index, time_slots)\n",
    "    print(f\"Decoded action {action_index}: task_id = {task_id}, time_slot = {time_slot}\")\n",
    "    \n",
    "    # Determine the start time for the time slot.\n",
    "    slot_start = get_timeslot_start(time_slot)\n",
    "    slot_end = slot_start + timedelta(minutes=30)  # assuming 30-minute duration\n",
    "    print(f\"Time slot {time_slot} corresponds to {slot_start.time()} - {slot_end.time()}\")\n",
    "    \n",
    "    # Update the task if it exists.\n",
    "    if task_id < len(state['tasks']):\n",
    "        state['tasks'][task_id]['scheduled_time'] = slot_start\n",
    "        print(f\"Task {task_id} scheduled at {slot_start}\")\n",
    "    else:\n",
    "        print(f\"Warning: Task ID {task_id} not found in the state.\")\n",
    "    \n",
    "    # Update the calendar: mark this time slot as occupied.\n",
    "    new_event = {\n",
    "        \"start\": slot_start,\n",
    "        \"end\": slot_end\n",
    "    }\n",
    "    state['calendar'].append(new_event)\n",
    "    print(f\"Added calendar event: {new_event}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Example usage:\n",
    "# Let's assume you have a real-world state dictionary similar to your training state.\n",
    "real_world_state = {\n",
    "    \"preferences\": {\n",
    "        \"productive_hours\": (9, 17)  # example: 9 AM to 5 PM\n",
    "    },\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"priority\": 2,\n",
    "            \"deadline\": datetime.now() + timedelta(days=2),\n",
    "            \"duration\": 60,  # in minutes\n",
    "            \"dependencies\": [],\n",
    "            \"type\": \"email\",\n",
    "        },\n",
    "        {\n",
    "            \"priority\": 1,\n",
    "            \"deadline\": datetime.now() + timedelta(days=3),\n",
    "            \"duration\": 120,\n",
    "            \"dependencies\": [],\n",
    "            \"type\": \"meeting\",\n",
    "            \"scheduled_time\": None\n",
    "        },\n",
    "        {\n",
    "            \"priority\": 3,\n",
    "            \"deadline\": datetime.now() + timedelta(days=1),\n",
    "            \"duration\": 30,\n",
    "            \"dependencies\": [],\n",
    "            \"type\": \"call\",\n",
    "            \"scheduled_time\": None\n",
    "        },\n",
    "        # Add more tasks if needed...\n",
    "    ],\n",
    "    \"calendar\": []  # initially empty\n",
    "}\n",
    "\n",
    "# Suppose your model selected action index 116.\n",
    "selected_action_index = 27\n",
    "updated_state = update_task_calendar(real_world_state, selected_action_index)\n",
    "\n",
    "# At this point, the `updated_state` has the task with task_id 2 updated with a scheduled time,\n",
    "# and the calendar now includes the corresponding event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ec8ff1-52c8-44c9-9930-2559adb42bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf94942e-e691-4b5a-ab3a-708311a638ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
